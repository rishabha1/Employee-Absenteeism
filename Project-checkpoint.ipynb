{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from fancyimpute import KNN   \n",
    "#import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "from random import randrange, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working directory\n",
    "os.chdir(\"C:/Users/Rishabh/Desktop/Edwisor-new/Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "absent_file = pd.read_excel(\"Absenteeism_at_work_Project.xls\")\n",
    "absent_file = absent_file.sort_values(by = [\"ID\",\"Reason for absence\"]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis\n",
    "categorical_index = [1,2,3,4,11,12,13,14,15,16] # these are the indices which store the categorical data\n",
    "numerical_index = [0,5,6,7,8,9,10,17,18,19,20] # these are the indices which store the numerical data\n",
    "for i in categorical_index :\n",
    "    absent_file.iloc[:,i] = absent_file.iloc[:,i].astype(\"category\")\n",
    "\n",
    "#for i in numerical_index :    #for converting the variables into numerical data, they must first be free of NAs.\n",
    " #   absent_file.iloc[:,i] = absent_file.iloc[:,i].astype(np.int64)      This step thus may be performed after the missing value and outlier analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #MISSING VALUE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe with missing percentage\n",
    "missing_val = pd.DataFrame(absent_file.isnull().sum())\n",
    "\n",
    "#Reset index\n",
    "missing_val = missing_val.reset_index()\n",
    "\n",
    "#Rename variable\n",
    "missing_val = missing_val.rename(columns = {'index': 'Variables', 0: 'Missing_percentage'})\n",
    "\n",
    "#Calculate percentage\n",
    "missing_val['Missing_percentage'] = (missing_val['Missing_percentage']/len(absent_file))*100\n",
    "\n",
    "#descending order\n",
    "missing_val = missing_val.sort_values('Missing_percentage', ascending = False).reset_index(drop = True)\n",
    "\n",
    "#save output results \n",
    "missing_val.to_csv(\"Miising_perc.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputation method\n",
    "#Actual value = 235\n",
    "#Mean = 221.0164\n",
    "#Median = 225\n",
    "#KNN = 235\n",
    "\n",
    "#create missing value\n",
    "absent_file['Transportation expense'].loc[3] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute with mean\n",
    "#absent_file['Transportation expense'] = absent_file['Transportation expense'].fillna(absent_file['Transportation expense'].mean())\n",
    "\n",
    "#Impute with median\n",
    "#absent_file['Transportation expense'] = absent_file['Transportation expense'].fillna(absent_file['Transportation expense'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply KNN imputation algorithm\n",
    "absent_file = pd.DataFrame(KNN(k = 3).complete(absent_file), columns = absent_file.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLIER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backup\n",
    "df = absent_file.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specific columns on which Outlier analysis needs to be applied. Rest all are data about the individual IDs.\n",
    "#And thus it would be unjustifiable to apply outlier analysis on it\n",
    "cnames_specific = [\"Work load Average/day \",\"Hit target\",\"Absenteeism time in hours\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Outlier analysis for the variables in cnames_specific\n",
    "    q75_wl, q25_wl = np.percentile(absent_file[\"Work load Average/day \"], [75 ,25])\n",
    "    iqr_wl = q75_wl - q25_wl\n",
    "    min_wl = q25_wl - (iqr_wl*1.5)\n",
    "    max_wl = q75_wl + (iqr_wl*1.5)\n",
    "    \n",
    "    q75_ht, q25_ht = np.percentile(absent_file[\"Hit target\"], [75 ,25])\n",
    "    iqr_ht = q75_ht - q25_ht\n",
    "    min_ht = q25_ht - (iqr_ht*1.5)\n",
    "    max_ht = q75_ht + (iqr_ht*1.5)\n",
    "    \n",
    "    q75_at, q25_at = np.percentile(absent_file[\"Absenteeism time in hours\"], [75 ,25])\n",
    "    iqr_at = q75_at - q25_at\n",
    "    min_at = q25_at - (iqr_at*1.5)\n",
    "    max_at = q75_wl + (iqr_at*1.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect and replace with NA\n",
    "absent_file.loc[absent_file[\"Work load Average/day \"] < min_wl , :'Work load Average/day ']=np.nan\n",
    "absent_file.loc[absent_file[\"Work load Average/day \"] < max_wl , :'Work load Average/day ']=np.nan\n",
    "\n",
    "absent_file.loc[absent_file[\"Hit target\"] < min_ht , :\"Hit target\"]=np.nan\n",
    "absent_file.loc[absent_file[\"Hit target\"] < min_ht , :\"Hit target\"]=np.nan\n",
    "    \n",
    "absent_file.loc[absent_file[\"Absenteeism time in hours\"] < min_at , :\"Absenteeism time in hours\"]=np.nan\n",
    "absent_file.loc[absent_file[\"Absenteeism time in hours\"] < min_at , :\"Absenteeism time in hours\"]=np.nan\n",
    "\n",
    "# #Calculate missing value\n",
    "# missing_val = pd.DataFrame(absent_file.isnull().sum())\n",
    "\n",
    "##Impute with KNN\n",
    "absent_file = pd.DataFrame(KNN(k = 3).complete(absent_file), columns = absent_file.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnames1 = ['ID','Transportation expense','Distance from Residence to Work','Service time','Age','Work load Average/day ','Hit target','Weight','Height','Body mass index','Absenteeism time in hours']\n",
    "##Correlation analysis\n",
    "#Correlation plot\n",
    "df_corr = absent_file.loc[:,cnames1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the width and height of the plot\n",
    "f,ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "#Generate correlation matrix\n",
    "corr = df_corr.corr()\n",
    "\n",
    "#Plot using seaborn library\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absent_file1 = absent_file.drop('Body mass index',axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = absent_file1.copy()\n",
    "#absent_file1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normality check\n",
    "%matplotlib inline  \n",
    "plt.hist(absent_file1['Work load Average/day '], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnames = [\"Transportation expense\",\"Distance from Residence to Work\",\"Service time\",\"Age\",\"Work load Average/day \",\"Hit target\",\"Weight\",\"Height\",\"Absenteeism time in hours\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nomalisation\n",
    "for i in cnames:\n",
    "    print(i)\n",
    "    absent_file1[i] = (absent_file1[i] - min(absent_file1[i]))/(max(absent_file1[i]) - min(absent_file1[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STORING PRE PROCESSED FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'absent_file1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d36142244f58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mabsent_file1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pre_processed.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'absent_file1' is not defined"
     ]
    }
   ],
   "source": [
    "absent_file1.to_csv(\"pre_processed.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Regression\n",
    "#Load Libraries\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the ddata into train and test\n",
    "train, test = train_test_split(absent_file1, test_size =0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree for regression\n",
    "fit = DecisionTreeRegressor(max_depth=2).fit(train.iloc[:,0:18], train.iloc[:,18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply model on test data\n",
    "predictions_DT = fit_DT.predict(test.iloc[:,0:18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating MAE\n",
    "mean_absolute_error(test.iloc[:,18], predictions_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAE = 12.9%\n",
    "#Accuracy = 87.1%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
